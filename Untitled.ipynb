{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import logging\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "class StanfordNLP:\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port,\n",
    "                                   timeout=30000)  # , quiet=False, logging_level=logging.DEBUG)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,depparse,dcoref,relation',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def word_tokenize(self, sentence):\n",
    "        return self.nlp.word_tokenize(sentence)\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        return self.nlp.pos_tag(sentence)\n",
    "\n",
    "    def ner(self, sentence):\n",
    "        return self.nlp.ner(sentence)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        return self.nlp.parse(sentence)\n",
    "\n",
    "    def dependency_parse(self, sentence):\n",
    "        return self.nlp.dependency_parse(sentence)\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return json.loads(self.nlp.annotate(sentence, properties=self.props))\n",
    "\n",
    "    def change_moji(self, sentence):\n",
    "        text = re.sub('\\(', '<', sentence)\n",
    "        text = re.sub('\\)', '>', text)\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_dict(_tokens):\n",
    "        tokens = defaultdict(dict)\n",
    "        for token in _tokens:\n",
    "            tokens[int(token['index'])] = {\n",
    "                'word': token['word'],\n",
    "                'lemma': token['lemma'],\n",
    "                'pos': token['pos'],\n",
    "                'ner': token['ner']\n",
    "            }\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_review = []\n",
    "    train_label = []\n",
    "    splited_sentence = []\n",
    "    parsed_sentence = []\n",
    "    group_list = []\n",
    "    test = []\n",
    "\n",
    "    sNLP = StanfordNLP()\n",
    "    \n",
    "    with open(\"D:/ruin/data/train_neg_full.json\") as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        train_data = json_data['data']\n",
    "\n",
    "    for a in range(len(train_data)):\n",
    "        train_review.append(train_data[a]['txt'])\n",
    "        train_label.append(train_data[a]['label'])\n",
    "\n",
    "    df_train_data = pd.DataFrame(train_review, columns=['data'])\n",
    "    df_train_data['label'] = train_label\n",
    "    \n",
    "    for a in df_train_data[9930:9940]['data']:\n",
    "        split_sentence = sent_tokenize(a)\n",
    "        splited_sentence.append(split_sentence)\n",
    "        \n",
    "    print(len(splited_sentence))\n",
    "        \n",
    "#     with open('tt.json', 'w', encoding='utf-8') as make_file:\n",
    "#         data = {}\n",
    "#         count = 0\n",
    "\n",
    "#         for a in splited_sentence:\n",
    "#             parsed_sentence_lst = []\n",
    "\n",
    "#             for b in a:\n",
    "#                 parse1 = \" \".join(sNLP.parse(b).split())\n",
    "#                 parse1 = sNLP.change_moji(parse1)\n",
    "#                 parsed_sentence_lst.append(parse1)\n",
    "#                 data['parsed_sentence'] = parsed_sentence_lst\n",
    "#             test.append(parsed_sentence_lst)\n",
    "#             count = count + 1\n",
    "#             print(count)\n",
    "#             # per_20 = count % 20\n",
    "#             # if per_20 == 0:\n",
    "#             #     print(count)\n",
    "\n",
    "#         data['splited_sentence'] = splited_sentence\n",
    "#         data['parsed_sentence'] = test\n",
    "\n",
    "#         json.dump(data, make_file, ensure_ascii=False, indent='\\t')\n",
    "#         make_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Story of a man who has unnatural feelings for a pig.', 'Starts out with a opening scene that is a terrific example of absurd comedy.', 'A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it is singers.', 'Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting.', 'Even those from the era should be turned off.', 'The cryptic dialogue would make Shakespeare seem easy to a third grader.', 'On a technical level it is better than you might think with some good cinematography by future great Vilmos Zsigmond.', 'Future stars Sally Kirkland and Frederic Forrest can be seen briefly.']\n"
     ]
    }
   ],
   "source": [
    "print(splited_sentence[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
